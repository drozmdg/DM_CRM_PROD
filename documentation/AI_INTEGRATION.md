# AI Integration Guide

This document provides a detailed explanation of the AI Assistant feature, its architecture, and how it uses CRM data to provide contextual awareness.

## 1. Overview

The AI Assistant is a chat-based interface that allows users to ask natural language questions about their CRM data. The assistant is designed to provide insightful, data-driven answers by leveraging a dynamically generated system prompt that gives it a comprehensive overview of the current state of the business.

## 2. Architecture and Data Flow

The AI chat feature is built on a client-server architecture that uses a local or remote Large Language Model (LLM) via an API.

1.  **Frontend (`AIChat.tsx`):** The user interacts with a chat interface. When a message is sent, it makes an API call to the backend.
2.  **Backend (`routes.ts`):** The backend receives the user's message and prepares to call the LLM.
3.  **Context Generation (`context.ts`):** Before calling the LLM, the backend runs the `generateSystemPrompt()` function. This function queries the database for all relevant CRM data (customers, processes, products, etc.) and formats it into a detailed text block.
4.  **LLM API Call (`api.ts`):** The backend sends the user's message, the chat history, and the dynamically generated system prompt to the LLM.
5.  **Response:** The LLM uses the rich context from the system prompt to generate an informed answer, which is then streamed back to the user's interface.

This architecture ensures that the AI has the most up-to-date information possible with every interaction, without the need for fine-tuning or a complex retrieval-augmented generation (RAG) pipeline.

## 3. The System Prompt

The key to the AI Assistant's effectiveness is the **dynamic system prompt**. This is a carefully crafted piece of text that is automatically generated and sent to the LLM with every request. It serves as the AI's "short-term memory" and provides a rich, detailed overview of the entire CRM dataset.

### 3.1. Purpose of the System Prompt

*   **Provide Context:** It gives the AI a snapshot of the current state of the business, including customer phases, process statuses, and product portfolios.
*   **Ensure Accuracy:** By providing real-time data, it helps the AI answer questions accurately without hallucinating details.
*   **Define the Persona:** It instructs the AI to act as a helpful CRM assistant and sets the tone for the interaction.
*   **Guide the Output:** It provides formatting guidelines (e.g., use markdown, tables) to ensure the responses are clear and readable.

### 3.2. Structure of the System Prompt

The system prompt is generated by the `generateSystemPrompt()` function in `server/lib/ai-chat/context.ts` and is structured into several key sections:

1.  **Introduction:** A brief statement defining the AI's role as a CRM assistant.

2.  **Summary Statistics:** A high-level, quantitative overview of the business, including:
    *   **Customer Stats:** Total active customers and a breakdown by their current phase.
    *   **Process Stats:** Total processes and distributions by status, SDLC stage, and functional area.
    *   **Product Stats:** Total products and distributions by therapeutic area, drug class, and regulatory status.
    *   **Other Stats:** Counts of teams, services, contacts, and documents.

3.  **Detailed Customer Information:** A list of all active customers, including their current phase and a count of their associated processes, teams, services, and products.

4.  **Pharmaceutical Product Portfolio:** A detailed breakdown of all pharmaceutical products, grouped by therapeutic area.

5.  **Cross-Team Product Management:** A list of any products that are managed by more than one team, highlighting areas of shared responsibility.

6.  **Upcoming Contract Renewals:** A list of all customers whose contracts are due to expire within the next 90 days.

7.  **Response Guidelines:** A set of instructions for the AI on how to format its answers, what tone to use, and what kind of information to include.

### 3.3. Example System Prompt

Below is an example of what a generated system prompt might look like. This is sent to the LLM with every user message.

```
You are an AI assistant for the Sales Dashboard CRM system.
You have full access to the CRM data since you're running locally and the user is the sole administrator.

## SUMMARY STATISTICS

### Customer Statistics
- Total Customers: 5
- Customer Phase Distribution: Steady State: 3, New Activation: 2
- Upcoming Contract Renewals: 1 in the next 90 days

### Process Statistics
- Total Processes: 12
- Process Status Distribution: Completed: 8, In Progress: 3, Not Started: 1
- SDLC Stage Distribution: Maintenance: 8, Deployment: 3, Requirements: 1

## DETAILED CUSTOMER INFORMATION
Sigma Pharma (Phase: Steady State, Processes: 3, Teams: 2, Services: 2, Products: 2)
BioGen Corp (Phase: New Activation, Processes: 2, Teams: 1, Services: 1, Products: 1)
... (and so on for all customers)

## UPCOMING CONTRACT RENEWALS
BioGen Corp (2025-08-15)

## RESPONSE GUIDELINES
- When answering questions, provide specific and detailed information about the CRM data.
- Format your responses using markdown for better readability.
- ... (and so on)
```

## 4. Configuration

The AI Assistant can be configured via the **AIConfig** component in the UI. This allows the user to:

*   Select the LLM model to use (if multiple are available).
*   Set the system prompt (though it is dynamically generated by default).
*   Adjust other model parameters like temperature and max tokens.

These settings are managed in `server/lib/ai-chat/config.ts`.
