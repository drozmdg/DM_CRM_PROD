#!/usr/bin/env node

/**
 * Supabase to PostgreSQL Data Migration Script
 * Extracts data from Supabase and loads it into PostgreSQL container
 */

import fs from 'fs';
import path from 'path';
import { createClient } from '@supabase/supabase-js';
import pg from 'pg';
import dotenv from 'dotenv';

// Load environment variables
dotenv.config({ path: './server/.env' });
dotenv.config({ path: './.env' }); // Also load root .env

const { Pool } = pg;

// Supabase configuration
const supabaseUrl = process.env.SUPABASE_URL;
const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
const supabaseAnonKey = process.env.SUPABASE_ANON_KEY;

console.log('ðŸ”„ Supabase to PostgreSQL Data Migration');
console.log(`ðŸ“¤ Source: ${supabaseUrl}`);

// PostgreSQL configuration
const pgConfig = {
  host: process.env.POSTGRES_HOST || 'localhost',
  port: process.env.POSTGRES_PORT || 5432,
  database: process.env.POSTGRES_DB || 'sales_dashboard_dev',
  user: process.env.POSTGRES_USER || 'postgres',
  password: process.env.POSTGRES_PASSWORD || 'postgres_dev_password',
};

console.log(`ðŸ“¥ Target: ${pgConfig.host}:${pgConfig.port}/${pgConfig.database}`);

const pgPool = new Pool(pgConfig);

// Initialize Supabase client (will be set in main function)
let supabase;

// Create migration directory
const migrationDir = path.join(process.cwd(), 'database', 'migration-data');
fs.mkdirSync(migrationDir, { recursive: true });

// Tables to migrate (in dependency order)
const tablesToMigrate = [
  'users',
  'teams',
  'customers',
  'contacts',
  'services',
  'processes',
  'documents',
  'process_team',
  'customer_notes',
  'customer_important_dates',
  'process_tasks',
  'process_milestones',
  'process_file_transfers',
  'process_notifications',
  'ai_chat_sessions',
  'ai_chat_messages'
];

/**
 * Test connections to both databases
 */
async function testConnections() {
  console.log('\nðŸ” Testing database connections...');
  
  try {
    // Test Supabase connection
    const { data, error } = await supabase.from('users').select('count').limit(1);
    if (error) {
      console.error('âŒ Supabase connection failed:', error.message);
      return false;
    }
    console.log('âœ… Supabase connection successful');
    
    // Test PostgreSQL connection
    const client = await pgPool.connect();
    await client.query('SELECT NOW()');
    client.release();
    console.log('âœ… PostgreSQL connection successful');
    
    return true;
  } catch (error) {
    console.error('âŒ Connection test failed:', error.message);
    return false;
  }
}

/**
 * Get table record count from Supabase
 */
async function getSupabaseTableCount(tableName) {
  try {
    const { count, error } = await supabase
      .from(tableName)
      .select('*', { count: 'exact', head: true });
    
    if (error) {
      console.log(`   âš ï¸  Table ${tableName} not accessible: ${error.message}`);
      return 0;
    }
    
    return count || 0;
  } catch (error) {
    console.log(`   âš ï¸  Error counting ${tableName}: ${error.message}`);
    return 0;
  }
}

/**
 * Extract data from Supabase table
 */
async function extractTableData(tableName) {
  try {
    console.log(`   ðŸ“¤ Extracting ${tableName}...`);
    
    // Get count first
    const count = await getSupabaseTableCount(tableName);
    console.log(`   ðŸ“Š ${tableName}: ${count} records`);
    
    if (count === 0) {
      return [];
    }
    
    // Extract all data
    const { data, error } = await supabase
      .from(tableName)
      .select('*');
    
    if (error) {
      console.error(`   âŒ Failed to extract ${tableName}:`, error.message);
      return [];
    }
    
    console.log(`   âœ… Extracted ${data.length} records from ${tableName}`);
    
    // Save to file for backup
    const dataFile = path.join(migrationDir, `${tableName}.json`);
    fs.writeFileSync(dataFile, JSON.stringify(data, null, 2));
    
    return data;
  } catch (error) {
    console.error(`   âŒ Error extracting ${tableName}:`, error.message);
    return [];
  }
}

/**
 * Load data into PostgreSQL table
 */
async function loadTableData(tableName, data) {
  if (!data || data.length === 0) {
    console.log(`   â­ï¸  Skipping ${tableName} - no data`);
    return true;
  }
  
  try {
    console.log(`   ðŸ“¥ Loading ${data.length} records into ${tableName}...`);
    
    const client = await pgPool.connect();
    
    try {
      // Check if table exists
      const tableCheck = await client.query(`
        SELECT EXISTS (
          SELECT FROM information_schema.tables 
          WHERE table_schema = 'public' 
          AND table_name = $1
        )
      `, [tableName]);
      
      if (!tableCheck.rows[0].exists) {
        console.log(`   âš ï¸  Table ${tableName} does not exist in PostgreSQL - skipping`);
        return true;
      }
      
      // Get table columns
      const columnsResult = await client.query(`
        SELECT column_name, data_type 
        FROM information_schema.columns 
        WHERE table_schema = 'public' 
        AND table_name = $1
        ORDER BY ordinal_position
      `, [tableName]);
      
      const columns = columnsResult.rows.map(row => row.column_name);
      console.log(`   ðŸ“‹ Target columns: ${columns.join(', ')}`);
      
      // Clear existing data
      await client.query(`DELETE FROM ${tableName}`);
      console.log(`   ðŸ§¹ Cleared existing data from ${tableName}`);
      
      // Prepare and execute inserts
      let insertedCount = 0;
      
      for (const record of data) {
        // Filter record to only include existing columns
        const filteredRecord = {};
        columns.forEach(col => {
          if (record.hasOwnProperty(col)) {
            filteredRecord[col] = record[col];
          }
        });
        
        // Skip if no valid columns
        if (Object.keys(filteredRecord).length === 0) {
          continue;
        }
        
        const recordColumns = Object.keys(filteredRecord);
        const values = Object.values(filteredRecord);
        const placeholders = values.map((_, i) => `$${i + 1}`).join(', ');
        
        const insertQuery = `
          INSERT INTO ${tableName} (${recordColumns.join(', ')}) 
          VALUES (${placeholders})
          ON CONFLICT DO NOTHING
        `;
        
        try {
          await client.query(insertQuery, values);
          insertedCount++;
        } catch (insertError) {
          console.log(`   âš ï¸  Insert error for record in ${tableName}: ${insertError.message}`);
          // Continue with other records
        }
      }
      
      console.log(`   âœ… Loaded ${insertedCount}/${data.length} records into ${tableName}`);
      
      // Reset sequences for tables with id columns
      if (columns.includes('id')) {
        try {
          await client.query(`
            SELECT setval(pg_get_serial_sequence('${tableName}', 'id'), 
                         COALESCE(MAX(id), 1)) 
            FROM ${tableName}
          `);
          console.log(`   ðŸ”„ Reset sequence for ${tableName}`);
        } catch (seqError) {
          console.log(`   âš ï¸  Sequence reset failed for ${tableName}: ${seqError.message}`);
        }
      }
      
      return true;
    } finally {
      client.release();
    }
  } catch (error) {
    console.error(`   âŒ Error loading ${tableName}:`, error.message);
    return false;
  }
}

/**
 * Verify data migration
 */
async function verifyMigration() {
  console.log('\nðŸ” Verifying data migration...');
  
  const migrationResults = [];
  
  for (const tableName of tablesToMigrate) {
    try {
      // Get Supabase count
      const supabaseCount = await getSupabaseTableCount(tableName);
      
      // Get PostgreSQL count
      const client = await pgPool.connect();
      const pgResult = await client.query(`SELECT COUNT(*) as count FROM ${tableName}`);
      const pgCount = parseInt(pgResult.rows[0].count);
      client.release();
      
      const result = {
        table: tableName,
        supabase: supabaseCount,
        postgres: pgCount,
        match: supabaseCount === pgCount,
        migrated: pgCount > 0
      };
      
      migrationResults.push(result);
      
      const status = result.match ? 'âœ…' : result.migrated ? 'âš ï¸' : 'âŒ';
      console.log(`   ${status} ${tableName}: Supabase(${supabaseCount}) â†’ PostgreSQL(${pgCount})`);
      
    } catch (error) {
      console.log(`   âŒ ${tableName}: Verification failed - ${error.message}`);
      migrationResults.push({
        table: tableName,
        supabase: 0,
        postgres: 0,
        match: false,
        migrated: false,
        error: error.message
      });
    }
  }
  
  return migrationResults;
}

/**
 * Generate migration report
 */
async function generateMigrationReport(results) {
  const reportPath = path.join(migrationDir, 'migration-report.md');
  
  const totalTables = results.length;
  const successfulMigrations = results.filter(r => r.match).length;
  const partialMigrations = results.filter(r => r.migrated && !r.match).length;
  const failedMigrations = results.filter(r => !r.migrated).length;
  
  const report = `# Supabase to PostgreSQL Data Migration Report

**Generated**: ${new Date().toISOString()}
**Source**: ${supabaseUrl}
**Target**: ${pgConfig.host}:${pgConfig.port}/${pgConfig.database}

## Migration Summary

- **Total Tables**: ${totalTables}
- **Successful Migrations**: ${successfulMigrations} (${((successfulMigrations/totalTables)*100).toFixed(1)}%)
- **Partial Migrations**: ${partialMigrations}
- **Failed Migrations**: ${failedMigrations}

## Detailed Results

| Table | Supabase | PostgreSQL | Status | Notes |
|-------|----------|------------|--------|--------|
${results.map(r => {
  const status = r.match ? 'âœ… Complete' : r.migrated ? 'âš ï¸ Partial' : 'âŒ Failed';
  const notes = r.error ? r.error : (r.match ? 'Perfect match' : r.migrated ? 'Count mismatch' : 'No data migrated');
  return `| ${r.table} | ${r.supabase} | ${r.postgres} | ${status} | ${notes} |`;
}).join('\n')}

## Migration Status

${successfulMigrations === totalTables ? `
### âœ… MIGRATION SUCCESSFUL

All ${totalTables} tables have been successfully migrated with perfect data matching.

**Task 4.1.3 Data Migration**: COMPLETE âœ…
**Production Readiness**: APPROVED
` : `
### ${successfulMigrations > totalTables/2 ? 'âš ï¸ MIGRATION PARTIAL' : 'âŒ MIGRATION INCOMPLETE'}

Migration completed with ${successfulMigrations}/${totalTables} tables fully migrated.

**Issues to Address**:
${results.filter(r => !r.match).map(r => `- ${r.table}: ${r.error || 'Data count mismatch'}`).join('\n')}

**Task 4.1.3 Data Migration**: ${successfulMigrations > totalTables/2 ? 'CONDITIONAL' : 'INCOMPLETE'}
`}

## Data Migration Procedures

### Backup Files Created:
${tablesToMigrate.map(table => `- \`${migrationDir}/${table}.json\``).join('\n')}

### PostgreSQL Data Verification:
\`\`\`sql
-- Verify record counts
${results.map(r => `SELECT '${r.table}' as table_name, COUNT(*) as record_count FROM ${r.table};`).join('\n')}
\`\`\`

### Rollback Procedure:
\`\`\`bash
# If migration needs to be reverted
${tablesToMigrate.reverse().map(table => `docker exec sales-dashboard-db-dev psql -U postgres -d sales_dashboard_dev -c "DELETE FROM ${table};"`).join('\n')}
\`\`\`

## Next Steps

1. **Verify Application Functionality**: Test application with migrated data
2. **Update Configuration**: Switch from Supabase to PostgreSQL in production
3. **Monitor Performance**: Ensure PostgreSQL performs adequately
4. **Backup Schedule**: Implement regular PostgreSQL backups
5. **Supabase Sunset**: Plan Supabase project shutdown after verification

---
*Report generated by Supabase to PostgreSQL Migration System*
*Task 4.1.3: Data Migration - Extract and load data: ${successfulMigrations === totalTables ? 'COMPLETE' : 'PARTIAL'}*
`;

  fs.writeFileSync(reportPath, report);
  console.log(`\nðŸ“‹ Migration report saved: ${reportPath}`);
  
  return successfulMigrations === totalTables;
}

/**
 * Main migration function
 */
async function runDataMigration() {
  console.log('\nðŸš€ Starting Supabase to PostgreSQL data migration...');
  
  // Test connections
  const connectionsOk = await testConnections();
  if (!connectionsOk) {
    console.error('âŒ Connection tests failed - aborting migration');
    process.exit(1);
  }
  
  // Migration process
  console.log('\nðŸ“¦ Starting data extraction and migration...');
  
  const migrationData = {};
  
  // Extract data from Supabase
  console.log('\nðŸ“¤ Extracting data from Supabase...');
  for (const tableName of tablesToMigrate) {
    migrationData[tableName] = await extractTableData(tableName);
  }
  
  // Load data into PostgreSQL
  console.log('\nðŸ“¥ Loading data into PostgreSQL...');
  for (const tableName of tablesToMigrate) {
    await loadTableData(tableName, migrationData[tableName]);
  }
  
  // Verify migration
  const results = await verifyMigration();
  
  // Generate report
  const success = await generateMigrationReport(results);
  
  console.log('\nðŸŽ‰ Data migration completed!');
  
  const totalTables = results.length;
  const successfulMigrations = results.filter(r => r.match).length;
  
  console.log(`ðŸ“Š Results: ${successfulMigrations}/${totalTables} tables successfully migrated`);
  
  if (success) {
    console.log('âœ… DATA MIGRATION SUCCESSFUL - All tables migrated perfectly');
    console.log('ðŸŽ¯ Task 4.1.3: Data Migration - Extract and load data: COMPLETE');
  } else {
    console.log('âš ï¸  DATA MIGRATION PARTIAL - Review migration report for details');
    console.log('ðŸ“‹ Some tables may need manual attention');
  }
  
  await pgPool.end();
  return success;
}

// Run data migration
runDataMigration().catch(error => {
  console.error('ðŸ’¥ Data migration failed:', error);
  process.exit(1);
});